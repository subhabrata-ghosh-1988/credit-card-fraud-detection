{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Credit_card_fraud_detection.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subhabrata-ghosh-1988/credit-card-fraud-detection/blob/main/Credit_card_fraud_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sTDizItsynB"
      },
      "source": [
        "## Credit Card Fraud Detection\n",
        "\n",
        "In this project you will predict fraudulent credit card transactions with the help of Machine learning models. Please import the following libraries to get started."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiL_6syYsynF"
      },
      "source": [
        "Fraudulent activities have increased severalfold, with around 52,304 cases of credit/debit card fraud reported in FY'19 alone. Due to this steep increase in banking frauds, it is the need of the hour to detect these fraudulent transactions in time in order to help consumers as well as banks, who are losing their credit worth each day. Every fraudulent credit card transaction that occur is a direct financial loss to the bank as the bank is responsible for the fraud transactions as well it also affects the overall customer satisfaction adversely.\n",
        "\n",
        "**The aim of this project is to identify and predict fraudulent credit card transactions using machine learning models.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcW2sp-OsynG"
      },
      "source": [
        "#### In order to complete the project, we are going to follow below high level steps to build and select best model.\n",
        "\n",
        " - Read the dataset and perform exploratory data analysis\n",
        " - Building different classification models on the unbalanced data\n",
        " - Building different models on 3 different balancing technique.\n",
        "      - Random Oversampling\n",
        "      - SMOTE\n",
        "      - ADASYN\n",
        "\n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "guPd1T2osynI",
        "outputId": "5681c0db-456a-4118-e9ff-a907353f2ecb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Importing python libraries :\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "# import machine learning and stats libraries:\n",
        "from scipy import stats\n",
        "from scipy.stats import norm, skew\n",
        "from scipy.special import boxcox1p\n",
        "from scipy.stats import boxcox_normmax\n",
        "\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve, auc \n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "from sklearn.linear_model import Ridge, Lasso, LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "# Import:\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "#install scikit-optimize\n",
        "!pip install scikit-optimize\n",
        "from skopt import BayesSearchCV\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.0.1)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (20.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdI7hfFmtP7Q"
      },
      "source": [
        "# To ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CawSZCfmtSs4",
        "outputId": "f722ab6a-0c4e-4c56-a77e-d966b3543a9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Download data from Kaggle and store it to the google drive \n",
        "\n",
        "! pip install -q kaggle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "! mkdir ~/.kaggle\n",
        "! cp /content/drive/'My Drive'/'Credit Card Fraud Assignment'/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d mlg-ulb/creditcardfraud\n",
        "!unzip creditcardfraud.zip\n",
        "#Create DataFrame from the csv file\n",
        "#df = pd.read_csv('creditcard.csv')\n",
        "#df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "cp: cannot stat '/content/drive/My Drive/Credit Card Fraud Assignment/kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/kaggle/api/kaggle_api_extended.py\", line 146, in authenticate\n",
            "    self.config_file, self.config_dir))\n",
            "IOError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n",
            "unzip:  cannot find or open creditcardfraud.zip, creditcardfraud.zip.zip or creditcardfraud.zip.ZIP.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSZPunabsynK"
      },
      "source": [
        "## Exploratory data analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5xMheu4synK"
      },
      "source": [
        "df = pd.read_csv('creditcard.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXsf4CBLsynL"
      },
      "source": [
        "#observe the different feature type present in the data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6YiFfPVsynN"
      },
      "source": [
        "Here we will observe the distribution of our classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqHZYR2JsynO"
      },
      "source": [
        "classes=df['Class'].value_counts()\n",
        "normal_share=classes[0]/df['Class'].count()*100\n",
        "fraud_share=classes[1]/df['Class'].count()*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX5W1Nb0synQ"
      },
      "source": [
        "# Create a bar plot for the number and percentage of fraudulent vs non-fraudulent transcations\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNAHB0rgsynR"
      },
      "source": [
        "# Create a scatter plot to observe the distribution of classes with time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm39BtZesynS"
      },
      "source": [
        "# Create a scatter plot to observe the distribution of classes with Amount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoPQf-AQsynT"
      },
      "source": [
        "# Drop unnecessary columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RJjaY4EsynU"
      },
      "source": [
        "### Splitting the data into train & test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7ZcGmhcsynV"
      },
      "source": [
        "y= #class variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diKkQMDmsynV"
      },
      "source": [
        "from sklearn import model_selection\n",
        "\n",
        "X_train, X_test, y_train, y_test = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klKIMHQhsynW"
      },
      "source": [
        "##### Preserve X_test & y_test to evaluate on the test data once you build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nfoJIAFsynW"
      },
      "source": [
        "print(np.sum(y))\n",
        "print(np.sum(y_train))\n",
        "print(np.sum(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEDkhABasynY"
      },
      "source": [
        "### Plotting the distribution of a variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuk4I2-6synh"
      },
      "source": [
        "# plot the histogram of a variable from the dataset to see the skewness"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMI60j8lsyni"
      },
      "source": [
        "### If there is skewness present in the distribution use:\n",
        "- <b>Power Transformer</b> package present in the <b>preprocessing library provided by sklearn</b> to make distribution more gaussian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAuyPu8psynj"
      },
      "source": [
        "# - Apply : preprocessing.PowerTransformer(copy=False) to fit & transform the train & test data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY036WO2synj"
      },
      "source": [
        "# plot the histogram of a variable from the dataset again to see the result "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FtlVcnTsynk"
      },
      "source": [
        "## Model Building\n",
        "- Build different models on the imbalanced dataset and see the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "8vvaTfiwsynk"
      },
      "source": [
        "# Logistic Regression\n",
        "from sklearn import linear_model #import the package\n",
        "\n",
        "num_C = ______  #--> list of values\n",
        "cv_num =   #--> list of values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD9gcm3csynl"
      },
      "source": [
        "#### perfom cross validation on the X_train & y_train to create:\n",
        "- X_train_cv\n",
        "- X_test_cv \n",
        "- y_train_cv\n",
        "- y_test_cv "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOkbwjVBsynl"
      },
      "source": [
        "#perform cross validation\n",
        "\n",
        "#perform hyperparameter tuning\n",
        "\n",
        "#print the evaluation result by choosing a evaluation metric\n",
        "\n",
        "#print the optimum value of hyperparameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKJkbymGsynm"
      },
      "source": [
        "### Similarly explore other algorithms by building models like:\n",
        "- KNN\n",
        "- SVM\n",
        "- Decision Tree\n",
        "- Random Forest\n",
        "- XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixfLH-lSsynm"
      },
      "source": [
        "#### Proceed with the model which shows the best result \n",
        "- Apply the best hyperparameter on the model\n",
        "- Predict on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na69wajdsynn"
      },
      "source": [
        "clf = ___  #initialise the model with optimum hyperparameters\n",
        "clf.fit(X_train, y_train)\n",
        "print --> #print the evaluation score on the X_test by choosing the best evaluation metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMb1u9YBsynn"
      },
      "source": [
        "### Print the important features of the best model to understand the dataset\n",
        "- This will not give much explanation on the already transformed dataset\n",
        "- But it will help us in understanding if the dataset is not PCA transformed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd4QAiMAsyno"
      },
      "source": [
        "var_imp = []\n",
        "for i in clf.feature_importances_:\n",
        "    var_imp.append(i)\n",
        "print('Top var =', var_imp.index(np.sort(clf.feature_importances_)[-1])+1)\n",
        "print('2nd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-2])+1)\n",
        "print('3rd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-3])+1)\n",
        "\n",
        "# Variable on Index-16 and Index-13 seems to be the top 2 variables\n",
        "top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-1])\n",
        "second_top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-2])\n",
        "\n",
        "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
        "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
        "\n",
        "np.random.shuffle(X_train_0)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = [20, 20]\n",
        "\n",
        "plt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')\n",
        "plt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index],\n",
        "            label='Actual Class-0 Examples')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eElgNH4Wsynp"
      },
      "source": [
        "## Model building with balancing Classes\n",
        "\n",
        "##### Perform class balancing with :\n",
        "- Random Oversampling\n",
        "- SMOTE\n",
        "- ADASYN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_zaUu0Tsynp"
      },
      "source": [
        "## Model Building\n",
        "- Build different models on the balanced dataset and see the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ezr0OiUnsynp"
      },
      "source": [
        "# Logistic Regression\n",
        "from sklearn import linear_model #import the package\n",
        "\n",
        "num_C = ______  #--> list of values\n",
        "cv_num =   #--> list of values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o666oNmIsynq"
      },
      "source": [
        "#### perfom cross validation on the X_train & y_train to create:\n",
        "- X_train_cv\n",
        "- X_test_cv \n",
        "- y_train_cv\n",
        "- y_test_cv "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taJ6VIaZsynq"
      },
      "source": [
        "### Random Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fezcqM8Rsynr"
      },
      "source": [
        "from imblearn import over_sampling #- import the packages\n",
        "\n",
        "#perform cross validation & then balance classes on X_train_cv & y_train_cv using Random Oversampling\n",
        "\n",
        "#perform hyperparameter tuning\n",
        "\n",
        "#print the evaluation result by choosing a evaluation metric\n",
        "\n",
        "#print the optimum value of hyperparameters\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNk3wdoesyns"
      },
      "source": [
        "### Similarly explore other algorithms on balanced dataset by building models like:\n",
        "- KNN\n",
        "- SVM\n",
        "- Decision Tree\n",
        "- Random Forest\n",
        "- XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WInjITCWsyns"
      },
      "source": [
        "### Print the class distribution after applying SMOTE "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl_m96Hpsynv"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "sm = over_sampling.SMOTE(random_state=0)\n",
        "X_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\n",
        "# Artificial minority samples and corresponding minority labels from SMOTE are appended\n",
        "# below X_train and y_train respectively\n",
        "# So to exclusively get the artificial minority samples from SMOTE, we do\n",
        "X_train_smote_1 = X_train_smote[X_train.shape[0]:]\n",
        "\n",
        "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
        "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
        "\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [20, 20]\n",
        "fig = plt.figure()\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
        "plt.scatter(X_train_smote_1[:X_train_1.shape[0], 0], X_train_smote_1[:X_train_1.shape[0], 1],\n",
        "            label='Artificial SMOTE Class-1 Examples')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
        "plt.scatter(X_train_0[:X_train_1.shape[0], 0], X_train_0[:X_train_1.shape[0], 1], label='Actual Class-0 Examples')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y97qJ7jGsynv"
      },
      "source": [
        "#perform cross validation & then balance classes on X_train_cv & y_train_cv using SMOTE\n",
        "\n",
        "#perform hyperparameter tuning\n",
        "\n",
        "#print the evaluation result by choosing a evaluation metric\n",
        "\n",
        "#print the optimum value of hyperparameters\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6oJwH1ssyny"
      },
      "source": [
        "##### Build models on other algorithms to see the better performing on SMOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf6Q3ElOsynz"
      },
      "source": [
        "### Print the class distribution after applying ADASYN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOG3ng09synz"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from imblearn import over_sampling\n",
        "\n",
        "ada = over_sampling.ADASYN(random_state=0)\n",
        "X_train_adasyn, y_train_adasyn = ada.fit_resample(X_train, y_train)\n",
        "# Artificial minority samples and corresponding minority labels from ADASYN are appended\n",
        "# below X_train and y_train respectively\n",
        "# So to exclusively get the artificial minority samples from ADASYN, we do\n",
        "X_train_adasyn_1 = X_train_adasyn[X_train.shape[0]:]\n",
        "\n",
        "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
        "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = [20, 20]\n",
        "fig = plt.figure()\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
        "plt.scatter(X_train_adasyn_1[:X_train_1.shape[0], 0], X_train_adasyn_1[:X_train_1.shape[0], 1],\n",
        "            label='Artificial ADASYN Class-1 Examples')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
        "plt.scatter(X_train_0[:X_train_1.shape[0], 0], X_train_0[:X_train_1.shape[0], 1], label='Actual Class-0 Examples')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAJYRxXusyn0"
      },
      "source": [
        "#perform cross validation & then balance classes on X_train_cv & y_train_cv using ADASYN\n",
        "\n",
        "#perform hyperparameter tuning\n",
        "\n",
        "#print the evaluation result by choosing a evaluation metric\n",
        "\n",
        "#print the optimum value of hyperparameters\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5B1VafLsyn0"
      },
      "source": [
        "##### Build models on other algorithms to see the better performing on ADASYN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT3Rd72Rsyn1"
      },
      "source": [
        "### Select the oversampling method which shows the best result on a model\n",
        "- Apply the best hyperparameter on the model\n",
        "- Predict on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9jTRtvfsyn3"
      },
      "source": [
        "# perform the best oversampling method on X_train & y_train\n",
        "\n",
        "clf = ___  #initialise the model with optimum hyperparameters\n",
        "clf.fit( ) # fit on the balanced dataset\n",
        "print() --> #print the evaluation score on the X_test by choosing the best evaluation metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKYy-PsCsyn3"
      },
      "source": [
        "### Print the important features of the best model to understand the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIdINqUfsyn4"
      },
      "source": [
        "var_imp = []\n",
        "for i in clf.feature_importances_:\n",
        "    var_imp.append(i)\n",
        "print('Top var =', var_imp.index(np.sort(clf.feature_importances_)[-1])+1)\n",
        "print('2nd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-2])+1)\n",
        "print('3rd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-3])+1)\n",
        "\n",
        "# Variable on Index-13 and Index-9 seems to be the top 2 variables\n",
        "top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-1])\n",
        "second_top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-2])\n",
        "\n",
        "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
        "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
        "\n",
        "np.random.shuffle(X_train_0)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = [20, 20]\n",
        "\n",
        "plt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')\n",
        "plt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index],\n",
        "            label='Actual Class-0 Examples')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfi3Qa5rsyn5"
      },
      "source": [
        "#### Print the FPR,TPR & select the best threshold from the roc curve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwfI9wNesyn6"
      },
      "source": [
        "print('Train auc =', metrics.roc_auc_score(_________)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(_________)\n",
        "threshold = thresholds[np.argmax(tpr-fpr)]\n",
        "print(threshold)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}